{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Chatbot/Query Search for Animals\n",
    "\n",
    "The goal for this notebook is to create a simple chatbot that can potentially answer various questions about animals.  To do this, we will scrape a variety of Wikipedia articles relating to animals, seperate out each sentence in each article, match the inputted query to the sentence that most closely matches the query via the TF-IDF method, and output that sentence as the chatbot's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webscrapping \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Utility\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering and Cleaning the Data\n",
    "\n",
    "First, we need a function to scrape the data from a Wikipedia link, which is written below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        wiki_text = \"\"\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        paragraphs = soup.find('div', {'id': 'mw-content-text'}).find_all('p')\n",
    "        for paragraph in paragraphs:\n",
    "            wiki_text += \" \" + paragraph.text\n",
    "        return wiki_text\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function, we can create a list of links that we want to scrape, and store the texts of all the articles for processing.  Below, you can see which Wikipedia articles we chose to scrape for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_wiki_pages = [\"https://en.wikipedia.org/wiki/Dog\",\n",
    "                      \"https://en.wikipedia.org/wiki/Cat\",\n",
    "                      \"https://en.wikipedia.org/wiki/Horse\",\n",
    "                      \"https://en.wikipedia.org/wiki/Lion\",\n",
    "                      \"https://en.wikipedia.org/wiki/Tiger\",\n",
    "                      \"https://en.wikipedia.org/wiki/Cattle\",\n",
    "                      \"https://en.wikipedia.org/wiki/Snake\",\n",
    "                      \"https://en.wikipedia.org/wiki/Insect\",\n",
    "                      \"https://en.wikipedia.org/wiki/Bear\",\n",
    "                      \"https://en.wikipedia.org/wiki/Wolf\",\n",
    "                      \"https://en.wikipedia.org/wiki/Koala\",\n",
    "                      \"https://en.wikipedia.org/wiki/Vertebrate\",\n",
    "                      \"https://en.wikipedia.org/wiki/Dinosaur\",\n",
    "                      \"https://en.wikipedia.org/wiki/Red_panda\",\n",
    "                      \"https://en.wikipedia.org/wiki/Capybara\",\n",
    "                      \"https://en.wikipedia.org/wiki/Reptile\",\n",
    "                      \"https://en.wikipedia.org/wiki/Giraffe\",\n",
    "                      \"https://en.wikipedia.org/wiki/Deer\",\n",
    "                      \"https://en.wikipedia.org/wiki/Cheetah\",\n",
    "                      \"https://en.wikipedia.org/wiki/Turtle\",\n",
    "                      \"https://en.wikipedia.org/wiki/Axolotl\",\n",
    "                      \"https://en.wikipedia.org/wiki/Chordate\",\n",
    "                      \"https://en.wikipedia.org/wiki/Shark\",\n",
    "                      \"https://en.wikipedia.org/wiki/Sloth\",\n",
    "                      \"https://en.wikipedia.org/wiki/Frog\",\n",
    "                      \"https://en.wikipedia.org/wiki/Quokka\",\n",
    "                      \"https://en.wikipedia.org/wiki/Gorilla\",\n",
    "                      \"https://en.wikipedia.org/wiki/Raccoon\",\n",
    "                      \"https://en.wikipedia.org/wiki/Leopard\",\n",
    "                      \"https://en.wikipedia.org/wiki/Otter\",\n",
    "                      \"https://en.wikipedia.org/wiki/Hippopotamus\",\n",
    "                      \"https://en.wikipedia.org/wiki/Narwhal\",\n",
    "                      \"https://en.wikipedia.org/wiki/Meerkat\",\n",
    "                      \"https://en.wikipedia.org/wiki/Sheep\",\n",
    "                      \"https://en.wikipedia.org/wiki/Goat\",\n",
    "                      \"https://en.wikipedia.org/wiki/Donkey\",\n",
    "                      \"https://en.wikipedia.org/wiki/Ferret\",\n",
    "                      \"https://en.wikipedia.org/wiki/Squirrel\",\n",
    "                      \"https://en.wikipedia.org/wiki/Amphibian\",\n",
    "                      \"https://en.wikipedia.org/wiki/Hamster\",\n",
    "                      \"https://en.wikipedia.org/wiki/Spider\",\n",
    "                      \"https://en.wikipedia.org/wiki/Platypus\",\n",
    "                      \"https://en.wikipedia.org/wiki/Lizard\",\n",
    "                      \"https://en.wikipedia.org/wiki/Hedgehog\",\n",
    "                      \"https://en.wikipedia.org/wiki/Glaucus_atlanticus\",\n",
    "                      \"https://en.wikipedia.org/wiki/Hyena\",\n",
    "                      \"https://en.wikipedia.org/wiki/Crocodile\",\n",
    "                      \"https://en.wikipedia.org/wiki/Rhinoceros\"]\n",
    "\n",
    "output_text = \"\"\n",
    "for link in list_of_wiki_pages:\n",
    "    output_text += \" \" + scrape_wikipedia(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we combined all the text from all articles into a single string.  In the next part we will break up the string into a new list where every element is a sentence from the string.  \n",
    "\n",
    "Before doing that, we need to do some simple text cleaning.  Note that this is NOT the part where we do the standard NLP preprocessing steps such as removing stopwords or lemmatizing; that part will come later.  This part is meant to clean up the formatting and remove messy text that carried over from scraping the data to make the eventual output more readable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['    The dog (Canis familiaris or Canis lupus familiaris) is a domesticated descendant of the wolf.',\n",
       " \"Also called the domestic dog, it is derived from extinct Pleistocene wolves, and the modern wolf is the dog's nearest living relative.\",\n",
       " 'The dog was the first species to be domesticated by humans.',\n",
       " 'Hunter-gatherers did this, over 15,000 years ago in Germany, which was before the development of agriculture.',\n",
       " 'Due to their long association with humans, dogs have expanded to a large number of domestic individuals and gained the ability to thrive on a starch-rich diet that would be inadequate for other canids.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = re.sub(r'\\[.*?\\]', '', output_text) # remove anything in square brackets\n",
    "output_text = output_text.replace(\"\\xa0\", \" \")    # fixing formatting for whatever \\xa0 is\n",
    "output_text = output_text.replace(\"\\n\", \" \")\n",
    "output_text = re.sub(r':\\u200a[0-9]+\\u200a ', '', output_text)\n",
    "output_text = re.sub(r':\\u200a[a-z]+\\u200a ', '', output_text)\n",
    "\n",
    "sentences = sent_tokenize(output_text)\n",
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first article was about dogs, the first five sentences in the example output above have to do with dogs.  You can verify that the sentences above are the first five sentences in the article about dogs at https://en.wikipedia.org/wiki/Dog.\n",
    "\n",
    "The sentences in `sentences` are going to be the outputs of our chatbot.  However, in order for the program to better understand which sentences match best with each query, the sentences need to be further preprocessed.  We do NOT want these preprocessed sentences as the output of the chatbot since preprocessing will include stripping away punctuation and stopwords, making the sentences difficult to understand and our chatbot will not produce realistic outputs.\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "We will write a function to do the preprocessing of each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dog canis familiaris canis lupus familiaris domesticate descendant wolf',\n",
       " 'also call domestic dog derive extinct pleistocene wolf modern wolf dog s near living relative',\n",
       " 'dog first specie domesticate human',\n",
       " 'hunter gatherers 15000 year ago germany development agriculture',\n",
       " 'due long association human dog expand large number domestic individual gain ability thrive starch rich diet would inadequate canid']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_formatter(input_string):\n",
    "    \n",
    "    # Tokenize\n",
    "    token_list = word_tokenize(input_string) \n",
    "    # Put to Lower Case\n",
    "    case_folded = [word.lower() for word in token_list] \n",
    "    # Remove Stop Words\n",
    "    stop_words_removed_case_folded = [word for word in case_folded if word not in stopwords.words('english')] \n",
    "    # Remove Punctuation\n",
    "    stop_words_punctuation_removed_case_folded = [word.replace(\"-\", \" \") for word in stop_words_removed_case_folded] # special case for hyphens\n",
    "    stop_words_punctuation_removed_case_folded = [re.sub(r'[^\\w\\s]', '', word) for word in stop_words_punctuation_removed_case_folded] \n",
    "    # POS Tagged\n",
    "    pos_tagged_words = nltk.pos_tag(stop_words_punctuation_removed_case_folded)\n",
    "\n",
    "    # POS tags for wnl.lematize\n",
    "    new_tags = [\"\"] * len(pos_tagged_words)\n",
    "    for i in range(len(pos_tagged_words)):\n",
    "        if ((pos_tagged_words[i][1] == \"VB\") | (pos_tagged_words[i][1] == \"VBG\") |\n",
    "            (pos_tagged_words[i][1] == \"VBD\") | (pos_tagged_words[i][1] == \"VBN\") |\n",
    "            (pos_tagged_words[i][1] == \"VBP\") | (pos_tagged_words[i][1] == \"VBZ\")):\n",
    "            new_tags[i] = wordnet.VERB\n",
    "        elif ((pos_tagged_words[i][1] == \"JJ\") | (pos_tagged_words[i][1] == \"JJR\") |\n",
    "              (pos_tagged_words[i][1] == \"JJS\")):\n",
    "            new_tags[i] = wordnet.ADJ\n",
    "        elif ((pos_tagged_words[i][1] == \"RB\") | (pos_tagged_words[i][1] == \"RBR\") |\n",
    "              (pos_tagged_words[i][1] == \"RBS\") | (pos_tagged_words[i][1] == \"WRB\")):\n",
    "            new_tags[i] = wordnet.ADV\n",
    "        else:\n",
    "            new_tags[i] = wordnet.NOUN\n",
    "\n",
    "    # Lemmatization\n",
    "    wnl = WordNetLemmatizer()\n",
    "    lemma_words = [\"\"] * len(stop_words_punctuation_removed_case_folded)\n",
    "    for i in range(len(stop_words_punctuation_removed_case_folded)):\n",
    "        lemma_words[i] = wnl.lemmatize(stop_words_punctuation_removed_case_folded[i], pos = new_tags[i])\n",
    "\n",
    "    sentence = \" \".join(lemma_words) \n",
    "    sentence = re.sub(r'\\s+', ' ', sentence).strip() # remove extra spaces between words\n",
    "    return sentence\n",
    "\n",
    "training_text = [\"\"] * len(sentences) # pre-allocate memory\n",
    "for i in range(len(training_text)):\n",
    "    training_text[i] = string_formatter(sentences[i])\n",
    "training_text[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing to the outputs of the original first five sentences, we can see that the above text is a lot more bare-bones.  This is because all the punctuation has been removed, stopwords have been removed, and each word has been lemmatized.\n",
    "\n",
    "## Creating the TF-IDF Matrix\n",
    "\n",
    "Now that we have our list of cleaned sentences, we can create our TF-IDF matrix.  A TF-IDF matrix is a matrix where each row corresponds to a sentence and each column corresponds to a word in the entire corpus.  Each element of the matrix has a measure that considers how often the word appears in the sentence and how many sentences the word appears in.  The formula to calculate the TF-IDF element value is $[1 + log_{10}(tf)]\\ *\\ log_{10}(\\frac{N}{df})$, there tf (term frequency) is how often the word appears in the sentence, df (document frequency) is the number of sentences the term appears in, and N is the total number of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(training_text).toarray()\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the elements of the TF-IDF matrix are mostly 0 (most sentences only contain a fraction of the words in the corpus), I will not output a part of the matrix as an example.\n",
    "\n",
    "## Outputting Results\n",
    "\n",
    "Now that we have our TF-IDF matrix, we can move onto the last part of matching a query to a sentence.  The way that this is done is by selecting the columns of the TF-IDF that contain the relevant words in the query and taking the rowsum of the remaining TF_IDF elements.  The element with the largest rowsum indicates the sentence with the highest relevance to the query.\n",
    "\n",
    "We will write a function to take in a query and output the most relevant sentence.  Note that this function will be calling on global variables defined throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animal_query_response(query):\n",
    "    tokenized_query = word_tokenize(string_formatter(query))\n",
    "\n",
    "    # Create a filter for the tfidf matrix that will filter out every row that does not correspond to a tokenized word from search_query\n",
    "    word_filter = np.zeros(len(feature_names)).astype(int)\n",
    "    for i, word in enumerate(feature_names):\n",
    "        if word in tokenized_query:\n",
    "            word_filter[i] = 1\n",
    "    word_filter = word_filter.astype(bool)\n",
    "\n",
    "    closest_sentence_index = tfidf_matrix[:, word_filter].sum(axis = 1).argmax()\n",
    "    return sentences[closest_sentence_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see what query results we get.  Note that we already know ahead of time that a lot of the query tests will likely not produce results that make sense.  We will discuss this in the \"Conclusion and Next Steps\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: dog breeds\n",
      "Output: In conformation shows, also referred to as breed shows, a judge familiar with the specific dog breed evaluates individual purebred dogs for conformity with their established breed type as described in the breed standard.\n",
      "\n",
      "Input: aniMals thAT ARE excellEnt Climbers\n",
      "Output: Deer are also excellent jumpers and swimmers.\n",
      "\n",
      "Input: What is the lifespan of an average house cat?\n",
      "Output: House cats often mate with feral cats.\n",
      "\n",
      "Input: Which animal is the fastest land mammal?\n",
      "Output: The cheetah is the world's fastest land animal.\n",
      "\n",
      "Input: What is the purpose of a chameleon's ability to change color?\n",
      "Output: The chameleons in general use their ability to change their coloration for signalling rather than camouflage, but some species such as Smith's dwarf chameleon do use active colour change for camouflage purposes.\n",
      "\n",
      "Input: How many hearts does an octopus have?\n",
      "Output: Bears do not have many predators.\n",
      "\n",
      "Input: Which big cat is known for its distinctive black mane?\n",
      "Output: Coyotes and big cats have also been known to attack dogs.\n",
      "\n",
      "Input: Which snake is the longest in the world?\n",
      "Output: In the Western world, some snakes are kept as pets, especially docile species such as the ball python and corn snake.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def query_input_output(query):\n",
    "    print(\"Input:\", query)\n",
    "    print(\"Output:\", animal_query_response(query))\n",
    "    print()\n",
    "\n",
    "queries = [\"dog breeds\",\n",
    "           \"aniMals thAT ARE excellEnt Climbers\",\n",
    "           \"What is the lifespan of an average house cat?\",\n",
    "           \"Which animal is the fastest land mammal?\",\n",
    "           \"What is the purpose of a chameleon's ability to change color?\",\n",
    "           \"How many hearts does an octopus have?\",\n",
    "           \"Which big cat is known for its distinctive black mane?\",\n",
    "           \"Which snake is the longest in the world?\"]\n",
    "\n",
    "for query in queries:\n",
    "    query_input_output(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of our results do not match up very well (which we will talk about), but other queries such as \"Which animal is the fastest land mammal?\" have a perfect response in \"The cheetah is the world's fastest land animal\".  For the notebook I only outputted a select few examples, but it is not much more work to make this into a chatbot by putting the function through a continuous loop that consistently asks for an input and returns an output.\n",
    "\n",
    "## Conclusion and Next Steps\n",
    "\n",
    "Given that the chatbot only considers the number of words in each sentence and gives zero weight to the context of a query, it makes sense that a lot of the prompts did not produce a strong result.  For example, the question \"What is the lifespan of an average house cat?\" produced the answer \"House cats often mate with feral cats\".  This is because the words in the query matched best with the words in the sentence.  When we look through the article on cats, it does have the sentence \"The average lifespan of pet cats has risen in recent decades\".  However, the sentence \"House cats often mate with feral cats\" contains the word \"house\", which also appeared in the query.  It is therefore likely that the TF-IDF matrix gave more weight to \"house\" than it did \"lifespan\", likely because of how often each word occurs in other documents (recall that the TF-IDF matrix considers how many documents the word appears in as well).  Furthermore, the original sentence also contains the word \"cat\" twice, whereas the other option only contains the word \"cat\" once.\n",
    "\n",
    "Clearly, context matters.  Next steps for this chatbot would likely be to move on from the TF-IDF method and implement contextual embeddings such as BERT or GPT.  At that point we could also consider using the Wikipedia articles to train our own generative answers, instead of simply outputting the most correct sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCN8010_classic_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
